# TMprompt
 The effectiveness of prompt-based fine-tuning has been demonstrated in several few-shot learning tasks. By formulating suitable templates and choosing representative label mappings, prompt learning can be used as a linguistic tool for exploiting the hidden knowledge of pre-trained language models (PLMs). However, designing prompt seems an expert-first or calculation-first thing since the lack of the suitable designing strategy. Motivated by the idea of student-teacher interaction, we offer two designing strategies for fast prompt learning from the prompt's semantic.  Specifically, we make use of a prompt generated from task-specific semantic dependency tree and a prompt defined by the metadata description of current task. What is more, we propose a sparse-labels Mapping strategy which allow a token-degree mapping relaxation, and thus decrease the experimental effort to find the best label mapping and enhance the model's ability to focus on different label mappings. Our results show that the proposed strategies exhibits state-of-the-art performance in few-shot learning tasks, which proves that the semantic-driven prompt could assume as a better knowledge probe to motivate PLMs.
